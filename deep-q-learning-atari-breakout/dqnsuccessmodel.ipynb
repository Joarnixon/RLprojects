{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":61179,"sourceType":"modelInstanceVersion","modelInstanceId":51141},{"sourceId":61201,"sourceType":"modelInstanceVersion","modelInstanceId":51162},{"sourceId":61253,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":51206},{"sourceId":61304,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":51248},{"sourceId":61390,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":51311},{"sourceId":61576,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":51453},{"sourceId":61577,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":51454},{"sourceId":61637,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":51502}],"dockerImageVersionId":30732,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":9136.464571,"end_time":"2024-06-05T17:54:28.785458","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-06-05T15:22:12.320887","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gym==0.26.2 gym[atari]==0.26.2","metadata":{"id":"va4WvaAEhrQP","outputId":"be2bf63d-6fbd-4149-a57c-679f9ca00b6c","papermill":{"duration":17.871618,"end_time":"2024-06-05T15:22:33.541076","exception":false,"start_time":"2024-06-05T15:22:15.669458","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-06T12:26:41.581776Z","iopub.execute_input":"2024-06-06T12:26:41.582161Z","iopub.status.idle":"2024-06-06T12:26:59.162645Z","shell.execute_reply.started":"2024-06-06T12:26:41.582132Z","shell.execute_reply":"2024-06-06T12:26:59.161167Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: gym==0.26.2 in /opt/conda/lib/python3.10/site-packages (0.26.2)\nRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gym==0.26.2) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym==0.26.2) (2.2.1)\nRequirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from gym==0.26.2) (0.0.8)\nCollecting ale-py~=0.8.0 (from gym[atari]==0.26.2)\n  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.8.0->gym[atari]==0.26.2) (6.1.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.8.0->gym[atari]==0.26.2) (4.9.0)\nDownloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m15.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: ale-py\nSuccessfully installed ale-py-0.8.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install autorom[accept-rom-license]\n!pip install gym[atari,accept-rom-license]==0.26.2","metadata":{"id":"fT_4CRQ93VQd","outputId":"939ebff6-d4b0-4e7d-cc43-21ab6d8e036a","papermill":{"duration":51.121145,"end_time":"2024-06-05T15:23:24.674788","exception":false,"start_time":"2024-06-05T15:22:33.553643","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-06T12:26:59.165298Z","iopub.execute_input":"2024-06-06T12:26:59.165724Z","iopub.status.idle":"2024-06-06T12:27:49.375124Z","shell.execute_reply.started":"2024-06-06T12:26:59.165657Z","shell.execute_reply":"2024-06-06T12:27:49.373632Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting autorom[accept-rom-license]\n  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]) (8.1.7)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]) (2.32.3)\nCollecting AutoROM.accept-rom-license (from autorom[accept-rom-license])\n  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]) (2024.2.2)\nDownloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\nBuilding wheels for collected packages: AutoROM.accept-rom-license\n  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446659 sha256=13993d50d51495367b70c62401dad5630bd559aa4e933e4b152d0f66826bf06a\n  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\nSuccessfully built AutoROM.accept-rom-license\nInstalling collected packages: AutoROM.accept-rom-license, autorom\nSuccessfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.6.1\nRequirement already satisfied: gym==0.26.2 in /opt/conda/lib/python3.10/site-packages (from gym[accept-rom-license,atari]==0.26.2) (0.26.2)\nRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gym==0.26.2->gym[accept-rom-license,atari]==0.26.2) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym==0.26.2->gym[accept-rom-license,atari]==0.26.2) (2.2.1)\nRequirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from gym==0.26.2->gym[accept-rom-license,atari]==0.26.2) (0.0.8)\nCollecting autorom~=0.4.2 (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.26.2)\n  Downloading AutoROM-0.4.2-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: ale-py~=0.8.0 in /opt/conda/lib/python3.10/site-packages (from gym[accept-rom-license,atari]==0.26.2) (0.8.1)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.8.0->gym[accept-rom-license,atari]==0.26.2) (6.1.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.8.0->gym[accept-rom-license,atari]==0.26.2) (4.9.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.26.2) (8.1.7)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.26.2) (2.32.3)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.26.2) (4.66.4)\nRequirement already satisfied: AutoROM.accept-rom-license in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.26.2) (0.6.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.26.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.26.2) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.26.2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.26.2) (2024.2.2)\nDownloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\nInstalling collected packages: autorom\n  Attempting uninstall: autorom\n    Found existing installation: AutoROM 0.6.1\n    Uninstalling AutoROM-0.6.1:\n      Successfully uninstalled AutoROM-0.6.1\nSuccessfully installed autorom-0.4.2\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Downloading an environment, resolving lots of problems is like 10% of the time","metadata":{"id":"abootR4icyeE","papermill":{"duration":0.018074,"end_time":"2024-06-05T15:23:29.042465","exception":false,"start_time":"2024-06-05T15:23:29.024391","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import gym\nimport matplotlib.pyplot as plt\nimport os\nimport random\nimport torch\nimport copy\nimport torch.nn as nn\nimport numpy as np\nfrom torch.optim import Adam\nfrom itertools import count\nfrom collections import deque\nos.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\nfrom gym.wrappers.monitoring import video_recorder","metadata":{"id":"yHYi5FPfbuPR","papermill":{"duration":4.879886,"end_time":"2024-06-05T15:23:33.940814","exception":false,"start_time":"2024-06-05T15:23:29.060928","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-06T12:27:49.376961Z","iopub.execute_input":"2024-06-06T12:27:49.377344Z","iopub.status.idle":"2024-06-06T12:27:53.430893Z","shell.execute_reply.started":"2024-06-06T12:27:49.377309Z","shell.execute_reply":"2024-06-06T12:27:53.429395Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"code","source":"env = gym.make(\"BreakoutNoFrameskip-v4\", render_mode='rgb_array')","metadata":{"id":"r2erfkahbxr8","papermill":{"duration":0.5284,"end_time":"2024-06-05T15:23:34.486458","exception":false,"start_time":"2024-06-05T15:23:33.958058","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-06T12:27:53.434291Z","iopub.execute_input":"2024-06-06T12:27:53.434978Z","iopub.status.idle":"2024-06-06T12:27:53.980128Z","shell.execute_reply.started":"2024-06-06T12:27:53.434932Z","shell.execute_reply":"2024-06-06T12:27:53.978997Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n[Powered by Stella]\n","output_type":"stream"}]},{"cell_type":"code","source":"device = 'cpu'","metadata":{"id":"ia5oUYgbmdbz","papermill":{"duration":0.026999,"end_time":"2024-06-05T15:23:34.530696","exception":false,"start_time":"2024-06-05T15:23:34.503697","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-06T12:27:53.981527Z","iopub.execute_input":"2024-06-06T12:27:53.982042Z","iopub.status.idle":"2024-06-06T12:27:53.988714Z","shell.execute_reply.started":"2024-06-06T12:27:53.981995Z","shell.execute_reply":"2024-06-06T12:27:53.987172Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":"## Start with preprocessing states","metadata":{"id":"M1JUMDbumP6t","papermill":{"duration":0.016874,"end_time":"2024-06-05T15:23:34.564916","exception":false,"start_time":"2024-06-05T15:23:34.548042","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from gym.core import ObservationWrapper\nimport cv2\nfrom torchvision.transforms.functional import resize\nfrom torchvision.transforms.functional import crop\n# will use this built-in class","metadata":{"id":"yJ8Frc-qmt_s","papermill":{"duration":1.989363,"end_time":"2024-06-05T15:23:36.572202","exception":false,"start_time":"2024-06-05T15:23:34.582839","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-06T12:27:53.990245Z","iopub.execute_input":"2024-06-06T12:27:53.990739Z","iopub.status.idle":"2024-06-06T12:27:55.817576Z","shell.execute_reply.started":"2024-06-06T12:27:53.990658Z","shell.execute_reply":"2024-06-06T12:27:55.816265Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class Preprocessing(ObservationWrapper):\n    def __init__(self, env):\n        super().__init__(env)\n\n        self.image_size = (64, 64)\n        self._gray_scale_rule = torch.tensor([[0.8, 0.1, 0.1]], dtype=torch.float32).to(device).T.unsqueeze(1).to(device)\n\n    def _gray_scale(self, img):\n        return torch.matmul(img, self._gray_scale_rule.reshape(-1, 1))\n\n    def observation(self, img):\n        img = img[90:200, 3:157]\n        img = cv2.resize(img, self.image_size, interpolation=cv2.INTER_AREA)\n        img = torch.tensor(img, dtype=torch.float32).to(device)\n        img = self._gray_scale(img).squeeze(-1)\n        img = img.unsqueeze(0) / 255\n        return img\n\n# 90: 200, 3:157 ","metadata":{"id":"55MvSsR_byeR","papermill":{"duration":0.032434,"end_time":"2024-06-05T15:23:36.621925","exception":false,"start_time":"2024-06-05T15:23:36.589491","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-06T12:27:55.819308Z","iopub.execute_input":"2024-06-06T12:27:55.820284Z","iopub.status.idle":"2024-06-06T12:27:55.831221Z","shell.execute_reply.started":"2024-06-06T12:27:55.820246Z","shell.execute_reply":"2024-06-06T12:27:55.829825Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Now incorporate some rules and preprocessing:","metadata":{"id":"LAj69b4_Eo_O","papermill":{"duration":0.017349,"end_time":"2024-06-05T15:23:36.656256","exception":false,"start_time":"2024-06-05T15:23:36.638907","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# code from openai/baselines but remaked due to version of gym\n\nclass FireResetEnv(gym.Wrapper):\n    def __init__(self, env):\n        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n        gym.Wrapper.__init__(self, env)\n        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    def reset(self, **kwargs):\n        self.env.reset(**kwargs)\n        obs, _, done, info, _ = self.env.step(1)\n        if done:\n            self.env.reset(**kwargs)\n        obs, _, done, info, _ = self.env.step(2)\n        if done:\n            self.env.reset(**kwargs)\n        return obs, info\n\nclass EpisodicLifeEnv(gym.Wrapper):\n    def __init__(self, env):\n        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        \"\"\"\n        gym.Wrapper.__init__(self, env)\n        self.lives = 0\n        self.was_real_done  = True\n\n    def step(self, action):\n        obs, reward, done, info, _ = self.env.step(action)\n        self.was_real_done = done\n        # check current lives, make loss of life terminal,\n        # then update lives to handle bonus lives\n        lives = self.env.unwrapped.ale.lives()\n        if lives < self.lives and lives > 0:\n            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n            # so it's important to keep lives > 0, so that we only reset once\n            # the environment advertises done.\n            done = True\n        self.lives = lives\n        return obs, reward, done, info, _\n\n    def reset(self, **kwargs):\n        \"\"\"Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        \"\"\"\n        if self.was_real_done:\n            obs, info = self.env.reset(**kwargs)\n        else:\n            # no-op step to advance from terminal/lost life state\n            obs, _, _, info, _ = self.env.step(0)\n        self.lives = self.env.unwrapped.ale.lives()\n        return obs, info\n\nclass MaxAndSkipEnv(gym.Wrapper):\n    def __init__(self, env, skip=4):\n        \"\"\"Return only every `skip`-th frame\"\"\"\n        gym.Wrapper.__init__(self, env)\n        # most recent raw observations (for max pooling across time steps)\n        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n        self._skip       = skip\n\n    def step(self, action):\n        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n        total_reward = 0.0\n        done = None\n        for i in range(self._skip):\n            obs, reward, done, info, _ = self.env.step(action)\n            if i == self._skip - 2: self._obs_buffer[0] = obs\n            if i == self._skip - 1: self._obs_buffer[1] = obs\n            total_reward += reward\n            if done:\n                break\n        # Note that the observation on the done=True frame\n        # doesn't matter\n        max_frame = self._obs_buffer.max(axis=0)\n\n        return max_frame, total_reward, done, info, _\n\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n\nclass ClipRewardEnv(gym.RewardWrapper):\n    def __init__(self, env):\n        gym.RewardWrapper.__init__(self, env)\n\n    def reward(self, reward):\n        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n        return np.sign(reward)\n","metadata":{"id":"XzrlYJFOLeCt","papermill":{"duration":0.043267,"end_time":"2024-06-05T15:23:36.716674","exception":false,"start_time":"2024-06-05T15:23:36.673407","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-06T12:27:55.832660Z","iopub.execute_input":"2024-06-06T12:27:55.833056Z","iopub.status.idle":"2024-06-06T12:27:55.856103Z","shell.execute_reply.started":"2024-06-06T12:27:55.833024Z","shell.execute_reply":"2024-06-06T12:27:55.854967Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from gym.wrappers import FrameStack\n\ndef AtariWrap(env):\n    # env = MaxAndSkipEnv(env, skip=2)\n    # env = FireResetEnv(env)\n    # env = ClipRewardEnv(env)\n    env = Preprocessing(env)\n    env = FrameStack(env, 2)\n    return env\nenv = AtariWrap(env)","metadata":{"id":"XGr6llJZAz6s","papermill":{"duration":0.074025,"end_time":"2024-06-05T15:23:36.807860","exception":false,"start_time":"2024-06-05T15:23:36.733835","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-06T12:27:55.857548Z","iopub.execute_input":"2024-06-06T12:27:55.857920Z","iopub.status.idle":"2024-06-06T12:27:55.918910Z","shell.execute_reply.started":"2024-06-06T12:27:55.857890Z","shell.execute_reply":"2024-06-06T12:27:55.917585Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"def process_observation(obs):\n    return torch.tensor(np.array([frame.to('cpu') for frame in obs._frames]), dtype=torch.float32).unsqueeze(0)","metadata":{"id":"MyqlHbSuvhbw","papermill":{"duration":0.028372,"end_time":"2024-06-05T15:23:36.853587","exception":false,"start_time":"2024-06-05T15:23:36.825215","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-06T12:27:55.923482Z","iopub.execute_input":"2024-06-06T12:27:55.924041Z","iopub.status.idle":"2024-06-06T12:27:55.930741Z","shell.execute_reply.started":"2024-06-06T12:27:55.924007Z","shell.execute_reply":"2024-06-06T12:27:55.929443Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Create a DeepLearning model which will get a sequence of 4 frames","metadata":{"id":"MBMIMhzZ8NEb","papermill":{"duration":0.017288,"end_time":"2024-06-05T15:23:36.888245","exception":false,"start_time":"2024-06-05T15:23:36.870957","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Agent(nn.Module):\n    def __init__(self, n_actions):\n        super().__init__()\n        self.n = n_actions\n\n        self.conv1 = nn.Conv3d(2, 4, kernel_size=(1, 8, 8), stride=2)\n        self.relu1 = nn.ReLU()\n        self.conv2 = nn.Conv3d(4, 8, kernel_size=(1, 4, 4), stride=2)\n        self.relu2 = nn.ReLU()\n        self.conv3 = nn.Conv3d(8, 4, kernel_size=(1, 3, 3))\n        self.relu3 = nn.ReLU()\n        self.conv4 = nn.Conv3d(4, 2, kernel_size=(1, 2, 2))\n        self.relu4 = nn.ReLU()\n    \n\n        self.bottleneck = nn.Flatten()\n\n        self.linear1 = nn.Linear(200, 10)\n        self.relu5 = nn.ReLU()\n        self.linear2 = nn.Linear(10, n_actions)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu1(x)\n        x = self.conv2(x)\n        x = self.relu2(x)\n        x = self.conv3(x)\n        x = self.relu3(x)\n        x = self.conv4(x)\n        x = self.relu4(x)\n        \n        x = self.bottleneck(x)\n        \n        x = self.linear1(x)\n        x = self.relu5(x)\n        x = self.linear2(x)\n        \n        return x\n\n    def select_action(self, observation, eps_greedy=True, eps=0.2):\n        if eps_greedy:\n            if random.random() > eps:\n                q = self(observation).squeeze(0)\n                max_q = torch.argmax(q)\n                return max_q\n            else:\n                return random.randint(0, self.n-1)\n        else:\n            q = self(observation).squeeze(0)\n            max_q = torch.argmax(q)\n            return max_q\n\n    def get_q_value(self, state, action):\n        if len(action.shape) == 0:\n            return self(state)[0][action.item()]\n        else:\n            return self(state).gather(dim=1, index=action)","metadata":{"id":"Ln3EKiDd8Mvx","papermill":{"duration":0.039697,"end_time":"2024-06-05T15:23:36.945252","exception":false,"start_time":"2024-06-05T15:23:36.905555","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-06T12:27:55.932296Z","iopub.execute_input":"2024-06-06T12:27:55.932754Z","iopub.status.idle":"2024-06-06T12:27:55.951476Z","shell.execute_reply.started":"2024-06-06T12:27:55.932709Z","shell.execute_reply":"2024-06-06T12:27:55.949894Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Custom loss function creation","metadata":{"papermill":{"duration":0.016563,"end_time":"2024-06-05T15:23:36.978794","exception":false,"start_time":"2024-06-05T15:23:36.962231","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def compute_q_loss(model, model_target, states, actions, rewards, next_states, dones, gamma, alpha):\n    if len(states.shape) != 5:\n        states = states.unsqueeze(0)\n        next_states = next_states.unsqueeze(0)\n\n    Q_target = torch.max(model_target(next_states.to(device)))\n        \n    first = model.get_q_value(states.to(device), actions.to(device))\n    second = rewards.to(device) + gamma * Q_target * dones.to(device)\n    return torch.clip(loss_fn(second, first), -1, 1)","metadata":{"id":"IPJ45Q7Vwzxt","papermill":{"duration":0.030546,"end_time":"2024-06-05T15:23:37.026334","exception":false,"start_time":"2024-06-05T15:23:36.995788","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-06T12:27:55.953200Z","iopub.execute_input":"2024-06-06T12:27:55.953603Z","iopub.status.idle":"2024-06-06T12:27:55.970623Z","shell.execute_reply.started":"2024-06-06T12:27:55.953571Z","shell.execute_reply":"2024-06-06T12:27:55.969143Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"## Load model to continue training","metadata":{"papermill":{"duration":0.016551,"end_time":"2024-06-05T15:23:37.105964","exception":false,"start_time":"2024-06-05T15:23:37.089413","status":"completed"},"tags":[]}},{"cell_type":"code","source":"def load_checkpoint(filepath):\n    checkpoint = torch.load(filepath)\n    model = checkpoint['model']\n    model.load_state_dict(checkpoint['state_dict'])\n    for parameter in model.parameters():\n        parameter.requires_grad = True\n    \n    model.eval()\n    \n    return model\n# model = load_checkpoint('/kaggle/input/newmodel18k/pytorch/ver1/1/checkpoint (31).pth')","metadata":{"papermill":{"duration":0.054829,"end_time":"2024-06-05T15:23:37.178277","exception":false,"start_time":"2024-06-05T15:23:37.123448","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-06T12:27:55.972973Z","iopub.execute_input":"2024-06-06T12:27:55.973823Z","iopub.status.idle":"2024-06-06T12:27:55.989281Z","shell.execute_reply.started":"2024-06-06T12:27:55.973771Z","shell.execute_reply":"2024-06-06T12:27:55.987981Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"## Training process","metadata":{"papermill":{"duration":0.017726,"end_time":"2024-06-05T15:23:37.214216","exception":false,"start_time":"2024-06-05T15:23:37.196490","status":"completed"},"tags":[]}},{"cell_type":"code","source":"model = Agent(env.action_space.n).to(device)\nloss_fn = nn.SmoothL1Loss()","metadata":{"id":"j1izR1aLAuoJ","papermill":{"duration":0.027427,"end_time":"2024-06-05T15:23:37.258711","exception":false,"start_time":"2024-06-05T15:23:37.231284","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-06T12:27:55.991318Z","iopub.execute_input":"2024-06-06T12:27:55.991805Z","iopub.status.idle":"2024-06-06T12:27:56.020919Z","shell.execute_reply.started":"2024-06-06T12:27:55.991750Z","shell.execute_reply":"2024-06-06T12:27:56.019831Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"code","source":"def train(env, model, episodes, eps_start=0.9, alpha=1e-3, gamma=0.99, buffer_size=100000, batch_size=32, target_model_update=10000):\n    n_done = 0\n    print(\"Started training\")\n    history = []\n    \n    model_target = copy.deepcopy(model).to(device)\n    model.train()\n    model.to(device)\n    optim = Adam(params=model.parameters(), lr=5e-5)\n    \n    \n    T = 0\n    total_reward = 0\n    eps = eps_start\n    \n    replay_buffer = deque(maxlen=buffer_size)\n    \n    while n_done < episodes:\n        state, info = env.reset()\n        state = process_observation(state).to(device)\n        \n        with torch.no_grad():\n            action = model.select_action(state, eps=eps)\n\n        for t in count(0, 1):\n            T += 1\n            next_state, reward, terminated, done, _info = env.step(action)\n            next_state = process_observation(next_state).to(device)\n            total_reward += reward\n            \n            replay_buffer.append((state.to('cpu'), torch.tensor([action], dtype=torch.int64), torch.tensor([reward], dtype=torch.float32), next_state.to('cpu'), torch.tensor([not terminated], dtype=torch.bool)))\n\n            if terminated:\n                history += [total_reward]\n                if n_done % 10 == 0:\n                    print(\"episode:\", n_done, 'completed by:', t, 'avg. reward:', np.mean(history))\n                    \n                if T > target_model_update:\n                    with torch.no_grad():\n                        model_target.load_state_dict(model.state_dict())\n                    T = 0\n                total_reward = 0\n                eps = np.clip(eps - 1/episodes, 0.1, 1)\n                n_done += 1\n                break\n                                                \n            with torch.no_grad():\n                next_action = model.select_action(next_state, eps=eps)\n            state = next_state\n            action = next_action\n            \n            if (n_done % 10 == 0) and t==100:\n                with torch.no_grad():\n                    print('q_values:', model(next_state))\n            \n            if len(replay_buffer) > 1000:\n                transitions = random.sample(replay_buffer, batch_size)\n                states = torch.as_tensor(np.asarray([t[0].squeeze(0) for t in transitions]), dtype=torch.float32)\n                actions = torch.as_tensor(np.asarray([t[1] for t in transitions]), dtype=torch.int64)\n                rewards = torch.as_tensor(np.asarray([t[2] for t in transitions]), dtype=torch.float32)\n                next_states = torch.as_tensor(np.asarray([t[3].squeeze(0) for t in transitions]), dtype=torch.float32)\n                dones = torch.as_tensor(np.asarray([t[4] for t in transitions]), dtype=torch.int64)\n                \n                loss = compute_q_loss(model, model_target, states, actions, rewards, next_states, dones, gamma, alpha)\n                optim.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_value_(model.parameters(), 2)\n                optim.step()\n    model.eval()\n    return model, history\n\n","metadata":{"id":"VwhTgigRoF3W","papermill":{"duration":0.046739,"end_time":"2024-06-05T15:23:37.322585","exception":false,"start_time":"2024-06-05T15:23:37.275846","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-06T12:27:56.022286Z","iopub.execute_input":"2024-06-06T12:27:56.022650Z","iopub.status.idle":"2024-06-06T12:27:56.046789Z","shell.execute_reply.started":"2024-06-06T12:27:56.022620Z","shell.execute_reply":"2024-06-06T12:27:56.045277Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"code","source":"model, history = train(env=env, model=model, episodes=250, eps_start=0.7, alpha=1e-3, gamma=0.99, buffer_size=100000, batch_size=32, target_model_update=2000)","metadata":{"papermill":{"duration":9049.361691,"end_time":"2024-06-05T17:54:26.702775","exception":false,"start_time":"2024-06-05T15:23:37.341084","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-06T12:27:56.048515Z","iopub.execute_input":"2024-06-06T12:27:56.049036Z","iopub.status.idle":"2024-06-06T12:28:03.557111Z","shell.execute_reply.started":"2024-06-06T12:27:56.048986Z","shell.execute_reply":"2024-06-06T12:28:03.554655Z"},"trusted":true},"execution_count":16,"outputs":[{"name":"stdout","text":"Started training\n","output_type":"stream"},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/gym/utils/passive_env_checker.py:233: DeprecationWarning: `np.bool8` is a deprecated alias for `np.bool_`.  (Deprecated NumPy 1.24)\n  if not isinstance(terminated, (bool, np.bool8)):\n","output_type":"stream"},{"name":"stdout","text":"q_values: tensor([[-0.1928,  0.1244,  0.3228,  0.2973]])\nepisode: 0 completed by: 790 avg. reward: 2.0\n","output_type":"stream"},{"name":"stderr","text":"\nKeyboardInterrupt\n\n","output_type":"stream"}]},{"cell_type":"code","source":"model.to('cpu')\nmodel.eval()\ncheckpoint = {'model': Agent(4), 'state_dict': model.state_dict()}\n\ntorch.save(checkpoint, 'checkpoint2.pth')","metadata":{"id":"CPndXaXJ1AzZ","papermill":{"duration":0.072946,"end_time":"2024-06-05T17:54:26.829992","exception":false,"start_time":"2024-06-05T17:54:26.757046","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-06T12:28:03.559130Z","iopub.status.idle":"2024-06-06T12:28:03.560005Z","shell.execute_reply.started":"2024-06-06T12:28:03.559538Z","shell.execute_reply":"2024-06-06T12:28:03.559566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.053456,"end_time":"2024-06-05T17:54:26.937985","exception":false,"start_time":"2024-06-05T17:54:26.884529","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.053697,"end_time":"2024-06-05T17:54:27.045756","exception":false,"start_time":"2024-06-05T17:54:26.992059","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.054004,"end_time":"2024-06-05T17:54:27.154364","exception":false,"start_time":"2024-06-05T17:54:27.100360","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.053931,"end_time":"2024-06-05T17:54:27.262688","exception":false,"start_time":"2024-06-05T17:54:27.208757","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.054022,"end_time":"2024-06-05T17:54:27.372785","exception":false,"start_time":"2024-06-05T17:54:27.318763","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.056073,"end_time":"2024-06-05T17:54:27.482874","exception":false,"start_time":"2024-06-05T17:54:27.426801","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}