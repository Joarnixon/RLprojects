{"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":60295,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":50431}],"dockerImageVersionId":30716,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"},"papermill":{"default_parameters":{},"duration":7188.647103,"end_time":"2024-06-03T15:15:42.570886","environment_variables":{},"exception":null,"input_path":"__notebook__.ipynb","output_path":"__notebook__.ipynb","parameters":{},"start_time":"2024-06-03T13:15:53.923783","version":"2.5.0"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install gym==0.26.2 gym[atari]==0.26.2","metadata":{"id":"va4WvaAEhrQP","outputId":"be2bf63d-6fbd-4149-a57c-679f9ca00b6c","papermill":{"duration":13.504293,"end_time":"2024-06-03T13:16:10.188690","exception":false,"start_time":"2024-06-03T13:15:56.684397","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-05T09:57:25.712946Z","iopub.execute_input":"2024-06-05T09:57:25.713420Z","iopub.status.idle":"2024-06-05T09:57:45.026764Z","shell.execute_reply.started":"2024-06-05T09:57:25.713385Z","shell.execute_reply":"2024-06-05T09:57:45.024733Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"Requirement already satisfied: gym==0.26.2 in /opt/conda/lib/python3.10/site-packages (0.26.2)\nRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gym==0.26.2) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym==0.26.2) (2.2.1)\nRequirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from gym==0.26.2) (0.0.8)\nCollecting ale-py~=0.8.0 (from gym[atari]==0.26.2)\n  Downloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.8.0->gym[atari]==0.26.2) (6.1.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.8.0->gym[atari]==0.26.2) (4.9.0)\nDownloading ale_py-0.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.7 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: ale-py\nSuccessfully installed ale-py-0.8.1\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install autorom[accept-rom-license]\n!pip install gym[atari,accept-rom-license]==0.26.2","metadata":{"id":"fT_4CRQ93VQd","outputId":"939ebff6-d4b0-4e7d-cc43-21ab6d8e036a","papermill":{"duration":40.27883,"end_time":"2024-06-03T13:16:50.476915","exception":false,"start_time":"2024-06-03T13:16:10.198085","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-05T09:57:45.030778Z","iopub.execute_input":"2024-06-05T09:57:45.031660Z","iopub.status.idle":"2024-06-05T09:58:39.011081Z","shell.execute_reply.started":"2024-06-05T09:57:45.031590Z","shell.execute_reply":"2024-06-05T09:58:39.009126Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting autorom[accept-rom-license]\n  Downloading AutoROM-0.6.1-py3-none-any.whl.metadata (2.4 kB)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]) (8.1.7)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]) (2.31.0)\nCollecting AutoROM.accept-rom-license (from autorom[accept-rom-license])\n  Downloading AutoROM.accept-rom-license-0.6.1.tar.gz (434 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m434.7/434.7 kB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Installing build dependencies ... \u001b[?25ldone\n\u001b[?25h  Getting requirements to build wheel ... \u001b[?25ldone\n\u001b[?25h  Preparing metadata (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autorom[accept-rom-license]) (2024.2.2)\nDownloading AutoROM-0.6.1-py3-none-any.whl (9.4 kB)\nBuilding wheels for collected packages: AutoROM.accept-rom-license\n  Building wheel for AutoROM.accept-rom-license (pyproject.toml) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for AutoROM.accept-rom-license: filename=AutoROM.accept_rom_license-0.6.1-py3-none-any.whl size=446659 sha256=90414f87d6fbe61c39e510c96d3f9d10540bf09a3383547e7f7c771f6d6fb46d\n  Stored in directory: /root/.cache/pip/wheels/6b/1b/ef/a43ff1a2f1736d5711faa1ba4c1f61be1131b8899e6a057811\nSuccessfully built AutoROM.accept-rom-license\n\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: AutoROM.accept-rom-license, autorom\nSuccessfully installed AutoROM.accept-rom-license-0.6.1 autorom-0.6.1\nRequirement already satisfied: gym==0.26.2 in /opt/conda/lib/python3.10/site-packages (from gym[accept-rom-license,atari]==0.26.2) (0.26.2)\nRequirement already satisfied: numpy>=1.18.0 in /opt/conda/lib/python3.10/site-packages (from gym==0.26.2->gym[accept-rom-license,atari]==0.26.2) (1.26.4)\nRequirement already satisfied: cloudpickle>=1.2.0 in /opt/conda/lib/python3.10/site-packages (from gym==0.26.2->gym[accept-rom-license,atari]==0.26.2) (2.2.1)\nRequirement already satisfied: gym-notices>=0.0.4 in /opt/conda/lib/python3.10/site-packages (from gym==0.26.2->gym[accept-rom-license,atari]==0.26.2) (0.0.8)\nRequirement already satisfied: ale-py~=0.8.0 in /opt/conda/lib/python3.10/site-packages (from gym[accept-rom-license,atari]==0.26.2) (0.8.1)\nCollecting autorom~=0.4.2 (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.26.2)\n  Downloading AutoROM-0.4.2-py3-none-any.whl.metadata (2.8 kB)\nRequirement already satisfied: importlib-resources in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.8.0->gym[accept-rom-license,atari]==0.26.2) (6.1.1)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from ale-py~=0.8.0->gym[accept-rom-license,atari]==0.26.2) (4.9.0)\nRequirement already satisfied: click in /opt/conda/lib/python3.10/site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.26.2) (8.1.7)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.26.2) (2.31.0)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.26.2) (4.66.4)\nRequirement already satisfied: AutoROM.accept-rom-license in /opt/conda/lib/python3.10/site-packages (from autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.26.2) (0.6.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.26.2) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.26.2) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.26.2) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->autorom~=0.4.2->autorom[accept-rom-license]~=0.4.2; extra == \"accept-rom-license\"->gym[accept-rom-license,atari]==0.26.2) (2024.2.2)\nDownloading AutoROM-0.4.2-py3-none-any.whl (16 kB)\n\u001b[33mWARNING: Error parsing requirements for aiohttp: [Errno 2] No such file or directory: '/opt/conda/lib/python3.10/site-packages/aiohttp-3.9.1.dist-info/METADATA'\u001b[0m\u001b[33m\n\u001b[0mInstalling collected packages: autorom\n  Attempting uninstall: autorom\n    Found existing installation: AutoROM 0.6.1\n    Uninstalling AutoROM-0.6.1:\n      Successfully uninstalled AutoROM-0.6.1\nSuccessfully installed autorom-0.4.2\n","output_type":"stream"}]},{"cell_type":"code","source":"! wget http://www.atarimania.com/roms/Roms.rar\n! python -m atari_py.import_roms ~/.kaggle/working/","metadata":{"papermill":{"duration":4.558548,"end_time":"2024-06-03T13:16:55.047740","exception":false,"start_time":"2024-06-03T13:16:50.489192","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-05T09:58:39.013356Z","iopub.execute_input":"2024-06-05T09:58:39.013824Z","iopub.status.idle":"2024-06-05T09:58:44.660309Z","shell.execute_reply.started":"2024-06-05T09:58:39.013784Z","shell.execute_reply":"2024-06-05T09:58:44.657785Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"--2024-06-05 09:58:40--  http://www.atarimania.com/roms/Roms.rar\nResolving www.atarimania.com (www.atarimania.com)... 212.83.163.6\nConnecting to www.atarimania.com (www.atarimania.com)|212.83.163.6|:80... connected.\nHTTP request sent, awaiting response... 301 Moved Permanently\nLocation: https://www.atarimania.com/roms/Roms.rar [following]\n--2024-06-05 09:58:40--  https://www.atarimania.com/roms/Roms.rar\nConnecting to www.atarimania.com (www.atarimania.com)|212.83.163.6|:443... connected.\nHTTP request sent, awaiting response... 200 OK\nLength: 19612325 (19M) [application/octet-stream]\nSaving to: 'Roms.rar'\n\nRoms.rar            100%[===================>]  18.70M  8.41MB/s    in 2.2s    \n\n2024-06-05 09:58:43 (8.41 MB/s) - 'Roms.rar' saved [19612325/19612325]\n\n/opt/conda/bin/python: Error while finding module specification for 'atari_py.import_roms' (ModuleNotFoundError: No module named 'atari_py')\n","output_type":"stream"}]},{"cell_type":"code","source":"! ale-import-roms ~/.kaggle/working/","metadata":{"papermill":{"duration":1.11972,"end_time":"2024-06-03T13:16:56.181104","exception":false,"start_time":"2024-06-03T13:16:55.061384","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-05T09:58:44.664514Z","iopub.execute_input":"2024-06-05T09:58:44.666438Z","iopub.status.idle":"2024-06-05T09:58:46.049421Z","shell.execute_reply.started":"2024-06-05T09:58:44.666374Z","shell.execute_reply":"2024-06-05T09:58:46.047799Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"Path /root/.kaggle/working doesn't exist.\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Downloading an environment, resolving lots of problems is like 20% of the time","metadata":{"id":"abootR4icyeE","papermill":{"duration":0.013286,"end_time":"2024-06-03T13:16:56.208358","exception":false,"start_time":"2024-06-03T13:16:56.195072","status":"completed"},"tags":[]}},{"cell_type":"code","source":"import gym\nimport matplotlib.pyplot as plt\nimport os\nimport random\nimport torch\nimport copy\nimport torch.nn as nn\nimport numpy as np\nfrom torch.optim import Adam\nfrom itertools import count\nfrom collections import deque, namedtuple\nos.environ[\"SDL_VIDEODRIVER\"] = \"dummy\"\nfrom gym.wrappers.monitoring import video_recorder","metadata":{"id":"yHYi5FPfbuPR","papermill":{"duration":3.834735,"end_time":"2024-06-03T13:17:00.056343","exception":false,"start_time":"2024-06-03T13:16:56.221608","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-05T09:58:46.052168Z","iopub.execute_input":"2024-06-05T09:58:46.053408Z","iopub.status.idle":"2024-06-05T09:58:49.779975Z","shell.execute_reply.started":"2024-06-05T09:58:46.053339Z","shell.execute_reply":"2024-06-05T09:58:49.778519Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"env = gym.make(\"BreakoutNoFrameskip-v4\", render_mode='rgb_array')","metadata":{"id":"r2erfkahbxr8","papermill":{"duration":0.403601,"end_time":"2024-06-03T13:17:00.473668","exception":false,"start_time":"2024-06-03T13:17:00.070067","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-05T09:58:49.784239Z","iopub.execute_input":"2024-06-05T09:58:49.785600Z","iopub.status.idle":"2024-06-05T09:58:50.392045Z","shell.execute_reply.started":"2024-06-05T09:58:49.785534Z","shell.execute_reply":"2024-06-05T09:58:50.390131Z"},"trusted":true},"execution_count":6,"outputs":[{"name":"stderr","text":"A.L.E: Arcade Learning Environment (version 0.8.1+53f58b7)\n[Powered by Stella]\n","output_type":"stream"}]},{"cell_type":"code","source":"device = 'cpu'","metadata":{"id":"ia5oUYgbmdbz","papermill":{"duration":0.097535,"end_time":"2024-06-03T13:17:00.584913","exception":false,"start_time":"2024-06-03T13:17:00.487378","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-05T09:58:50.394113Z","iopub.execute_input":"2024-06-05T09:58:50.394594Z","iopub.status.idle":"2024-06-05T09:58:50.401312Z","shell.execute_reply.started":"2024-06-05T09:58:50.394554Z","shell.execute_reply":"2024-06-05T09:58:50.399319Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"## Start with preprocessing states","metadata":{"id":"M1JUMDbumP6t","papermill":{"duration":0.013213,"end_time":"2024-06-03T13:17:00.675107","exception":false,"start_time":"2024-06-03T13:17:00.661894","status":"completed"},"tags":[]}},{"cell_type":"code","source":"from gym.core import ObservationWrapper\nimport cv2\nfrom torchvision.transforms.functional import resize\nfrom torchvision.transforms.functional import crop\n# will use this built-in class","metadata":{"id":"yJ8Frc-qmt_s","papermill":{"duration":1.327411,"end_time":"2024-06-03T13:17:02.015730","exception":false,"start_time":"2024-06-03T13:17:00.688319","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-05T09:58:50.423850Z","iopub.execute_input":"2024-06-05T09:58:50.424474Z","iopub.status.idle":"2024-06-05T09:58:52.365707Z","shell.execute_reply.started":"2024-06-05T09:58:50.424422Z","shell.execute_reply":"2024-06-05T09:58:52.364265Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"code","source":"class Preprocessing(ObservationWrapper):\n    def __init__(self, env):\n        super().__init__(env)\n\n        self.image_size = (64, 64)\n        self._gray_scale_rule = torch.tensor([[0.8, 0.1, 0.1]], dtype=torch.float32).to(device).T.unsqueeze(1).to(device)\n\n    def _gray_scale(self, img):\n        return torch.matmul(img, self._gray_scale_rule.reshape(-1, 1))\n\n    def observation(self, img):\n        img = img[30:200, 6:154]\n        img = cv2.resize(img, self.image_size, interpolation=cv2.INTER_AREA)\n        img = torch.tensor(img, dtype=torch.float32).to(device)\n        img = self._gray_scale(img).squeeze(-1)\n        img = torch.tensor(img, dtype=torch.float32).to(device).unsqueeze(0) / 255\n        return img\n\n","metadata":{"id":"55MvSsR_byeR","papermill":{"duration":0.024176,"end_time":"2024-06-03T13:17:02.053679","exception":false,"start_time":"2024-06-03T13:17:02.029503","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-05T09:58:52.371350Z","iopub.execute_input":"2024-06-05T09:58:52.371888Z","iopub.status.idle":"2024-06-05T09:58:52.383884Z","shell.execute_reply.started":"2024-06-05T09:58:52.371856Z","shell.execute_reply":"2024-06-05T09:58:52.382493Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"## Now incorporate some rules and preprocessing:","metadata":{"id":"LAj69b4_Eo_O","papermill":{"duration":0.013461,"end_time":"2024-06-03T13:17:02.080380","exception":false,"start_time":"2024-06-03T13:17:02.066919","status":"completed"},"tags":[]}},{"cell_type":"code","source":"# code from openai/baselines but remaked due to version of gym\n\nclass FireResetEnv(gym.Wrapper):\n    def __init__(self, env):\n        \"\"\"Take action on reset for environments that are fixed until firing.\"\"\"\n        gym.Wrapper.__init__(self, env)\n        assert env.unwrapped.get_action_meanings()[1] == 'FIRE'\n        assert len(env.unwrapped.get_action_meanings()) >= 3\n\n    def reset(self, **kwargs):\n        self.env.reset(**kwargs)\n        obs, _, done, info, _ = self.env.step(1)\n        if done:\n            self.env.reset(**kwargs)\n        obs, _, done, info, _ = self.env.step(2)\n        if done:\n            self.env.reset(**kwargs)\n        return obs, info\n\nclass EpisodicLifeEnv(gym.Wrapper):\n    def __init__(self, env):\n        \"\"\"Make end-of-life == end-of-episode, but only reset on true game over.\n        Done by DeepMind for the DQN and co. since it helps value estimation.\n        \"\"\"\n        gym.Wrapper.__init__(self, env)\n        self.lives = 0\n        self.was_real_done  = True\n\n    def step(self, action):\n        obs, reward, done, info, _ = self.env.step(action)\n        self.was_real_done = done\n        print(done)\n        # check current lives, make loss of life terminal,\n        # then update lives to handle bonus lives\n        lives = self.env.unwrapped.ale.lives()\n        if lives < self.lives and lives > 0:\n            # for Qbert sometimes we stay in lives == 0 condition for a few frames\n            # so it's important to keep lives > 0, so that we only reset once\n            # the environment advertises done.\n            done = True\n        self.lives = lives\n        return obs, reward, done, info, _\n\n    def reset(self, **kwargs):\n        \"\"\"Reset only when lives are exhausted.\n        This way all states are still reachable even though lives are episodic,\n        and the learner need not know about any of this behind-the-scenes.\n        \"\"\"\n        if self.was_real_done:\n            print(self.was_real_done)\n            obs, info = self.env.reset(**kwargs)\n        else:\n            # no-op step to advance from terminal/lost life state\n            obs, _, _, info, _ = self.env.step(0)\n        self.lives = self.env.unwrapped.ale.lives()\n        return obs, info\n\nclass MaxAndSkipEnv(gym.Wrapper):\n    def __init__(self, env, skip=4):\n        \"\"\"Return only every `skip`-th frame\"\"\"\n        gym.Wrapper.__init__(self, env)\n        # most recent raw observations (for max pooling across time steps)\n        self._obs_buffer = np.zeros((2,)+env.observation_space.shape, dtype=np.uint8)\n        self._skip       = skip\n\n    def step(self, action):\n        \"\"\"Repeat action, sum reward, and max over last observations.\"\"\"\n        total_reward = 0.0\n        done = None\n        for i in range(self._skip):\n            obs, reward, done, info, _ = self.env.step(action)\n            if i == self._skip - 2: self._obs_buffer[0] = obs\n            if i == self._skip - 1: self._obs_buffer[1] = obs\n            total_reward += reward\n            if done:\n                break\n        # Note that the observation on the done=True frame\n        # doesn't matter\n        max_frame = self._obs_buffer.max(axis=0)\n\n        return max_frame, total_reward, done, info, _\n\n    def reset(self, **kwargs):\n        return self.env.reset(**kwargs)\n\nclass ClipRewardEnv(gym.RewardWrapper):\n    def __init__(self, env):\n        gym.RewardWrapper.__init__(self, env)\n\n    def reward(self, reward):\n        \"\"\"Bin reward to {+1, 0, -1} by its sign.\"\"\"\n        return np.sign(reward)\n","metadata":{"id":"XzrlYJFOLeCt","papermill":{"duration":0.03465,"end_time":"2024-06-03T13:17:02.128465","exception":false,"start_time":"2024-06-03T13:17:02.093815","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-05T09:58:52.385815Z","iopub.execute_input":"2024-06-05T09:58:52.386213Z","iopub.status.idle":"2024-06-05T09:58:52.411563Z","shell.execute_reply.started":"2024-06-05T09:58:52.386181Z","shell.execute_reply":"2024-06-05T09:58:52.409864Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"code","source":"from gym.wrappers import FrameStack\n\ndef AtariWrap(env):\n    env = MaxAndSkipEnv(env, skip=2)\n    env = FireResetEnv(env)\n    env = ClipRewardEnv(env)\n    env = Preprocessing(env)\n    env = FrameStack(env, 4)\n    return env\nenv = AtariWrap(env)","metadata":{"id":"XGr6llJZAz6s","papermill":{"duration":0.226038,"end_time":"2024-06-03T13:17:02.368701","exception":false,"start_time":"2024-06-03T13:17:02.142663","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-05T09:58:52.413677Z","iopub.execute_input":"2024-06-05T09:58:52.414206Z","iopub.status.idle":"2024-06-05T09:58:52.473000Z","shell.execute_reply.started":"2024-06-05T09:58:52.414165Z","shell.execute_reply":"2024-06-05T09:58:52.471571Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"def process_observation(obs):\n    return torch.tensor(np.array([frame.to('cpu') for frame in obs._frames]), dtype=torch.float32).unsqueeze(0)","metadata":{"id":"MyqlHbSuvhbw","papermill":{"duration":0.020554,"end_time":"2024-06-03T13:17:02.472456","exception":false,"start_time":"2024-06-03T13:17:02.451902","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-05T10:00:23.994178Z","iopub.execute_input":"2024-06-05T10:00:23.995193Z","iopub.status.idle":"2024-06-05T10:00:24.002537Z","shell.execute_reply.started":"2024-06-05T10:00:23.995152Z","shell.execute_reply":"2024-06-05T10:00:24.000073Z"},"trusted":true},"execution_count":22,"outputs":[]},{"cell_type":"markdown","source":"## Create a DeepLearning model which will get a sequence of 4 frames","metadata":{"id":"MBMIMhzZ8NEb","papermill":{"duration":0.013655,"end_time":"2024-06-03T13:17:02.396131","exception":false,"start_time":"2024-06-03T13:17:02.382476","status":"completed"},"tags":[]}},{"cell_type":"code","source":"class Agent(nn.Module):\n    def __init__(self, n_actions):\n        super().__init__()\n        self.n = n_actions\n\n        self.conv1 = nn.Conv3d(4, 8, kernel_size=(1, 3, 3), stride=2)\n        self.relu1 = nn.ReLU()\n        self.conv2 = nn.Conv3d(8, 4, kernel_size=(1, 3, 3), stride=2)\n        self.relu2 = nn.ReLU()\n        self.conv3 = nn.Conv3d(4, 4, kernel_size=(1, 3, 3))\n        self.relu3 = nn.ReLU()\n        self.conv4 = nn.Conv3d(4, 2, kernel_size=(1, 2, 2))\n        self.relu4 = nn.ReLU()\n    \n\n        self.bottleneck = nn.Flatten()\n\n        self.linear1 = nn.Linear(288, 10)\n        self.relu5 = nn.ReLU()\n        self.linear2 = nn.Linear(10, n_actions)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        x = self.relu1(x)\n        x = self.conv2(x)\n        x = self.relu2(x)\n        x = self.conv3(x)\n        x = self.relu3(x)\n        x = self.conv4(x)\n        x = self.relu4(x)\n        \n        x = self.bottleneck(x)\n        \n        x = self.linear1(x)\n        x = self.relu5(x)\n        x = self.linear2(x)\n        \n        return x\n\n    def select_action(self, observation, eps_greedy=True, eps=0.2):\n        if eps_greedy:\n            if random.random() > eps:\n                q = self(observation).squeeze(0)\n                max_q = torch.argmax(q)\n                return max_q\n            else:\n                return random.randint(0, self.n-1)\n        else:\n            q = self(observation).squeeze(0)\n            max_q = torch.argmax(q)\n            return max_q\n\n    def get_q_value(self, state, action):\n        if len(action.shape) == 0:\n            return self(state)[0][action.item()]\n        else:\n            action = action.tolist()\n            return self(state)[range(0, len(action)), action]","metadata":{"id":"Ln3EKiDd8Mvx","papermill":{"duration":0.028842,"end_time":"2024-06-03T13:17:02.438542","exception":false,"start_time":"2024-06-03T13:17:02.409700","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-05T10:00:23.865427Z","iopub.execute_input":"2024-06-05T10:00:23.865900Z","iopub.status.idle":"2024-06-05T10:00:23.886548Z","shell.execute_reply.started":"2024-06-05T10:00:23.865867Z","shell.execute_reply":"2024-06-05T10:00:23.884699Z"},"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"markdown","source":"## Custom loss function creation","metadata":{}},{"cell_type":"code","source":"def compute_q_loss(model, model_target, states, actions, rewards, next_states, gamma, alpha):\n    if len(states.shape) != 5:\n        states = states.unsqueeze(0)\n        next_states = next_states.unsqueeze(0)\n    with torch.no_grad():\n        Q_target = torch.max(model_target(next_states.to(device)))\n        \n    first = model.get_q_value(states.to(device), actions)\n    second = rewards.to(device) + gamma * Q_target\n    return torch.clip(loss_fn(second, first), -1, 1)","metadata":{"id":"IPJ45Q7Vwzxt","papermill":{"duration":0.022333,"end_time":"2024-06-03T13:17:02.508668","exception":false,"start_time":"2024-06-03T13:17:02.486335","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-05T10:00:24.136751Z","iopub.execute_input":"2024-06-05T10:00:24.137168Z","iopub.status.idle":"2024-06-05T10:00:24.146135Z","shell.execute_reply.started":"2024-06-05T10:00:24.137136Z","shell.execute_reply":"2024-06-05T10:00:24.144704Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"code","source":"def compute_q_loss_terminal(model, states, actions, rewards, alpha):\n    first = model.get_q_value(states.to(device), actions)\n    second = rewards.to(device)\n    return torch.clip(loss_fn(second, first), -1, 1)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T10:00:24.297882Z","iopub.execute_input":"2024-06-05T10:00:24.298324Z","iopub.status.idle":"2024-06-05T10:00:24.305095Z","shell.execute_reply.started":"2024-06-05T10:00:24.298289Z","shell.execute_reply":"2024-06-05T10:00:24.303622Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"## Load model to continue training","metadata":{}},{"cell_type":"code","source":"def load_checkpoint(filepath):\n    checkpoint = torch.load(filepath)\n    model = checkpoint['model']\n    model.load_state_dict(checkpoint['state_dict'])\n    for parameter in model.parameters():\n        parameter.requires_grad = True\n    \n    model.eval()\n    \n    return model\n# model = load_checkpoint('/kaggle/input/dqn/pytorch/ver1/1/checkpoint (3).pth')","metadata":{"papermill":{"duration":0.189209,"end_time":"2024-06-03T13:17:02.711195","exception":false,"start_time":"2024-06-03T13:17:02.521986","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-05T10:00:26.262726Z","iopub.execute_input":"2024-06-05T10:00:26.263184Z","iopub.status.idle":"2024-06-05T10:00:26.272003Z","shell.execute_reply.started":"2024-06-05T10:00:26.263151Z","shell.execute_reply":"2024-06-05T10:00:26.270199Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## Training process","metadata":{}},{"cell_type":"code","source":"model = Agent(env.action_space.n).to(device)\nloss_fn = nn.MSELoss()","metadata":{"id":"j1izR1aLAuoJ","papermill":{"duration":0.025268,"end_time":"2024-06-03T13:17:02.756224","exception":false,"start_time":"2024-06-03T13:17:02.730956","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-05T10:00:27.121725Z","iopub.execute_input":"2024-06-05T10:00:27.122232Z","iopub.status.idle":"2024-06-05T10:00:27.133376Z","shell.execute_reply.started":"2024-06-05T10:00:27.122195Z","shell.execute_reply":"2024-06-05T10:00:27.131587Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"def train(env, model, episodes, eps_start=1, alpha=1e-3, gamma=0.99, buffer_size=100000, batch_size=32, target_model_update=10000):\n    n_done = 0\n    print(\"Started training\")\n    history = []\n    \n    model_target = copy.deepcopy(model).to(device)\n    model.train()\n    model.to(device)\n    optim = Adam(params=model.parameters(), lr=1e-4)\n    \n    \n    T = 0\n    eps = eps_start\n    \n    while n_done < episodes:\n        states_buffer = torch.empty(size=(1, 4, 1, 64, 64), dtype=torch.float32)\n        actions_buffer = torch.tensor([], dtype=torch.uint8)\n        rewards_buffer = torch.tensor([], dtype=torch.float32)\n        next_states_buffer = torch.empty(size=(1, 4, 1, 64, 64), dtype=torch.float32)\n        \n        total_reward = 0\n        \n        state, info = env.reset()\n        state = process_observation(state)\n        state = state.to(device)\n        with torch.no_grad():\n            action = model.select_action(state, eps=eps)\n\n        for t in count(0, 1):\n            T += 1\n            next_state, reward, terminated, done, _info = env.step(action)\n            next_state = process_observation(next_state).to(device)\n\n            if terminated:\n                loss = compute_q_loss_terminal(model, next_state, torch.tensor([action], dtype=torch.uint8), torch.tensor([reward], dtype=torch.float32), alpha)\n                optim.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_value_(model.parameters(), 2)\n                optim.step()\n                \n                history += [total_reward]\n                if n_done % 10 == 0:\n                    print(\"episode:\", n_done, 'completed by:', t, 'reward:', total_reward)\n                if T > target_model_update:\n                    with torch.no_grad():\n                        model_target = copy.deepcopy(model).to(device)\n                    T = 0\n                eps = np.clip(eps - 1/episodes, 0.1, 1)\n                n_done += 1\n                break\n                            \n            states_buffer = torch.cat((states_buffer, state.to('cpu')))\n            actions_buffer = torch.cat((actions_buffer, torch.tensor([action], dtype=torch.uint8)))\n            rewards_buffer = torch.cat((rewards_buffer, torch.tensor([reward], dtype=torch.float32)))\n            next_states_buffer = torch.cat((next_states_buffer, next_state.to('cpu')))\n                        \n            total_reward += reward\n            \n            with torch.no_grad():\n                next_action = model.select_action(next_state, eps=eps)\n\n            state = next_state\n            action = next_action\n            \n            loss = compute_q_loss(model, model_target, states_buffer[-1], actions_buffer[-1], rewards_buffer[-1], next_states_buffer[-1], gamma, alpha)\n            optim.zero_grad()\n            loss.backward()\n            torch.nn.utils.clip_grad_value_(model.parameters(), 2)\n            optim.step()\n            \n            if (n_done % 10 == 0) and t==100:\n                with torch.no_grad():\n                    print('q_values:', model(next_state), 'loss:', loss)\n            \n            if random.random() < 0.02 and len(states_buffer) > batch_size:\n                # there seems to be a bug with using state with index 0, no idea why\n                samples = np.random.randint(2, len(states_buffer) - 1, size=batch_size)\n                loss = compute_q_loss(model, model_target, states_buffer[samples], actions_buffer[samples], rewards_buffer[samples], next_states_buffer[samples], gamma, alpha)\n                optim.zero_grad()\n                loss.backward()\n                torch.nn.utils.clip_grad_value_(model.parameters(), 2)\n                optim.step()\n    model.eval()\n    return model, history\n\n","metadata":{"id":"VwhTgigRoF3W","papermill":{"duration":0.034609,"end_time":"2024-06-03T13:17:02.804317","exception":false,"start_time":"2024-06-03T13:17:02.769708","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-05T10:01:16.802956Z","iopub.execute_input":"2024-06-05T10:01:16.803456Z","iopub.status.idle":"2024-06-05T10:01:16.831592Z","shell.execute_reply.started":"2024-06-05T10:01:16.803417Z","shell.execute_reply":"2024-06-05T10:01:16.829993Z"},"trusted":true},"execution_count":31,"outputs":[]},{"cell_type":"code","source":"model, history = train(env=env, model=model, episodes=1000, eps_start=0.9, alpha=1e-3, gamma=0.99, buffer_size=10000, batch_size=32, target_model_update=10000)","metadata":{"execution":{"iopub.status.busy":"2024-06-05T10:01:17.359226Z","iopub.execute_input":"2024-06-05T10:01:17.359738Z","iopub.status.idle":"2024-06-05T10:01:34.474943Z","shell.execute_reply.started":"2024-06-05T10:01:17.359699Z","shell.execute_reply":"2024-06-05T10:01:34.472776Z"},"trusted":true},"execution_count":32,"outputs":[{"name":"stdout","text":"Started training\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_33/3927676706.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n  img = torch.tensor(img, dtype=torch.float32).to(device).unsqueeze(0) / 255\n","output_type":"stream"},{"name":"stdout","text":"q_values: tensor([[-0.0068,  0.1477,  0.1399,  0.1319]]) loss: tensor(1.8557e-06, grad_fn=<ClampBackward1>)\nepisode: 0 completed by: 443 reward: 2.0\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","Cell \u001b[0;32mIn[32], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m model, history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepisodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps_start\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.9\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgamma\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.99\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m32\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget_model_update\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[31], line 31\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(env, model, episodes, eps_start, alpha, gamma, buffer_size, batch_size, target_model_update)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m count(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     30\u001b[0m     T \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m---> 31\u001b[0m     next_state, reward, terminated, done, _info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m     next_state \u001b[38;5;241m=\u001b[39m process_observation(next_state)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m terminated:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gym/wrappers/frame_stack.py:173\u001b[0m, in \u001b[0;36mFrameStack.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    164\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m    165\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment, appending the observation to the frame buffer.\u001b[39;00m\n\u001b[1;32m    166\u001b[0m \n\u001b[1;32m    167\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;124;03m        Stacked observations, reward, terminated, truncated, and information from the environment\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    174\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes\u001b[38;5;241m.\u001b[39mappend(observation)\n\u001b[1;32m    175\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(\u001b[38;5;28;01mNone\u001b[39;00m), reward, terminated, truncated, info\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gym/core.py:384\u001b[0m, in \u001b[0;36mObservationWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m    383\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a modified observation using :meth:`self.observation` after calling :meth:`env.step`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 384\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobservation(observation), reward, terminated, truncated, info\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gym/core.py:418\u001b[0m, in \u001b[0;36mRewardWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m    417\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Modifies the reward using :meth:`self.reward` after the environment :meth:`env.step`.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 418\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    419\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m observation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreward(reward), terminated, truncated, info\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gym/core.py:319\u001b[0m, in \u001b[0;36mWrapper.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action: ActType) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[ObsType, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m]:\n\u001b[1;32m    318\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment with action.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 319\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[11], line 71\u001b[0m, in \u001b[0;36mMaxAndSkipEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     69\u001b[0m done \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skip):\n\u001b[0;32m---> 71\u001b[0m     obs, reward, done, info, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skip \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m2\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_buffer[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m i \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_skip \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_obs_buffer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m=\u001b[39m obs\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gym/wrappers/order_enforcing.py:37\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 37\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/gym/wrappers/env_checker.py:39\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/ale_py/env/gym.py:256\u001b[0m, in \u001b[0;36mAtariEnv.step\u001b[0;34m(self, action_ind)\u001b[0m\n\u001b[1;32m    254\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[1;32m    255\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(frameskip):\n\u001b[0;32m--> 256\u001b[0m     reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43male\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    257\u001b[0m is_terminal \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_over(with_truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m    258\u001b[0m is_truncated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39male\u001b[38;5;241m.\u001b[39mgame_truncated()\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "],"ename":"KeyboardInterrupt","evalue":"","output_type":"error"}]},{"cell_type":"code","source":"checkpoint = {'model': Agent(4), 'state_dict': model.state_dict()}\n\ntorch.save(checkpoint, 'checkpoint.pth')","metadata":{"id":"CPndXaXJ1AzZ","papermill":{"duration":0.074875,"end_time":"2024-06-03T15:15:40.653778","exception":false,"start_time":"2024-06-03T15:15:40.578903","status":"completed"},"tags":[],"execution":{"iopub.status.busy":"2024-06-05T09:58:53.845224Z","iopub.status.idle":"2024-06-05T09:58:53.845711Z","shell.execute_reply.started":"2024-06-05T09:58:53.845447Z","shell.execute_reply":"2024-06-05T09:58:53.845465Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.043886,"end_time":"2024-06-03T15:15:40.741161","exception":false,"start_time":"2024-06-03T15:15:40.697275","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.042969,"end_time":"2024-06-03T15:15:40.827382","exception":false,"start_time":"2024-06-03T15:15:40.784413","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.042946,"end_time":"2024-06-03T15:15:40.913654","exception":false,"start_time":"2024-06-03T15:15:40.870708","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.04329,"end_time":"2024-06-03T15:15:41.002789","exception":false,"start_time":"2024-06-03T15:15:40.959499","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.043085,"end_time":"2024-06-03T15:15:41.089005","exception":false,"start_time":"2024-06-03T15:15:41.045920","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"papermill":{"duration":0.045047,"end_time":"2024-06-03T15:15:41.191791","exception":false,"start_time":"2024-06-03T15:15:41.146744","status":"completed"},"tags":[]},"execution_count":null,"outputs":[]}]}